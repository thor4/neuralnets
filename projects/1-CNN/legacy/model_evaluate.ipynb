{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_evaluate.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOe3RLpJz8DBvY04Vi1UYN1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thor4/neuralnets/blob/master/projects/1-CNN/model_evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHcTgWXkB-4b"
      },
      "source": [
        "## Steps to reproduce `model.evaluate()` error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63uVilpRCI6P"
      },
      "source": [
        "### 1: Setup the model\n",
        "\n",
        "This model was created using Tensorflow's transfer learning tutorial. MobileNetV2 was used as the convolutional base. Gabors in two classes representing tilts from 45 degrees in the clockwise direction and tilts in the counterclockwise direction were used to train the final classification layer. Finally, the model was fine-tuned by training the weights for layers 101 onwards.\n",
        "\n",
        "Run the cell to download a zip file from OSF then extract its contents into the newly created directory: `content/18kim_range_ft/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpRtxiFmDo7x",
        "outputId": "3c3cf453-4f5b-433d-a737-fcbf86f77f9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# @title Download model\n",
        "\n",
        "import requests, os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "print(\"Start downloading and unzipping `Range model`...\")\n",
        "name = '18kim_range_ft'\n",
        "fname = f\"{name}.zip\"\n",
        "url = f\"https://osf.io/vbwsk/download\" #osf share link\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "with open(fname, 'wb') as fh:\n",
        "  fh.write(r.content) #download file\n",
        "\n",
        "with ZipFile(fname, 'r') as zfile:\n",
        "  zfile.extractall() #extract contents\n",
        "\n",
        "if os.path.exists(fname):\n",
        "  os.remove(fname) #delete zip file\n",
        "else:\n",
        "  print(f\"The file {fname} does not exist\")\n",
        "\n",
        "print(\"Download completed.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading and unzipping `Range model`...\n",
            "Download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOM5VyU8x80i"
      },
      "source": [
        "#### Load the model\n",
        "Next, we load the model using Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RVZI9Dexw2N",
        "outputId": "c21fe2d5-68f7-491f-9af4-ca890f643191",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf \n",
        "model = tf.keras.models.load_model('18kim_range_ft') #load previously trained fine-tuned range model (model 2)\n",
        "model.summary() #verify architecture, trainable: 2,225,153 params between last 100 layers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 160, 160, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.math.truediv (TFOpLambda) (None, 160, 160, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf.math.subtract (TFOpLambda (None, 160, 160, 3)       0         \n",
            "_________________________________________________________________\n",
            "mobilenetv2_1.00_160 (Functi (None, 5, 5, 1280)        2257984   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 1281      \n",
            "=================================================================\n",
            "Total params: 2,259,265\n",
            "Trainable params: 1,862,721\n",
            "Non-trainable params: 396,544\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZXJPKMezPAP"
      },
      "source": [
        "### 2: Download & load datasets to test model with\n",
        "Download the test datasets from OSF and extract the contents into the newly created directory: `content/datasets/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJiTI49Sz6yw",
        "outputId": "8025ff6c-1b62-4e8c-eca3-dfd426488a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Start downloading and unzipping `18 test datasets`...\")\n",
        "name = 'model2_dset1-18_cond'\n",
        "fname = f\"{name}.zip\"\n",
        "url = f\"https://osf.io/jkryf/download\" #osf share link\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "with open(fname, 'wb') as fh:\n",
        "  fh.write(r.content) #download file\n",
        "\n",
        "with ZipFile(fname, 'r') as zfile:\n",
        "  zfile.extractall(\"datasets\") #extract contents\n",
        "\n",
        "if os.path.exists(fname):\n",
        "  os.remove(fname) #delete zip file\n",
        "else:\n",
        "  print(f\"The file {fname} does not exist\")\n",
        "\n",
        "print(\"Download completed.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading and unzipping `18 test datasets`...\n",
            "Download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4rHfgAq0nbi"
      },
      "source": [
        "Load all 18 sets and use prefetch to streamline image loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVkzI7QC2gFL",
        "outputId": "7dcd4b02-2e91-4dd1-bcc4-d781cf8ef52c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "BATCH_SIZE = 32 \n",
        "IMG_SIZE = (160, 160) #forces a resize from 170x170 since MobileNetV2 has weights only for certain sizes\n",
        "AUTOTUNE = tf.data.AUTOTUNE #prompts the tf.data runtime to tune the value dynamically at runtime\n",
        "def model2_init_sets(BATCH_SIZE, IMG_SIZE, AUTOTUNE):\n",
        "    curr_dir = os.getcwd() \n",
        "    set1_dir = os.path.join(curr_dir, 'datasets/s1-t_0.1-c_0.3')\n",
        "    set2_dir = os.path.join(curr_dir, 'datasets/s2-t_0.1-c_0.45')\n",
        "    set3_dir = os.path.join(curr_dir, 'datasets/s3-t_0.1-c_1')\n",
        "    set4_dir = os.path.join(curr_dir, 'datasets/s4-t_0.2-c_0.3')\n",
        "    set5_dir = os.path.join(curr_dir, 'datasets/s5-t_0.2-c_0.45')\n",
        "    set6_dir = os.path.join(curr_dir, 'datasets/s6-t_0.2-c_1')\n",
        "    set7_dir = os.path.join(curr_dir, 'datasets/s7-t_0.4-c_0.3')\n",
        "    set8_dir = os.path.join(curr_dir, 'datasets/s8-t_0.4-c_0.45')\n",
        "    set9_dir = os.path.join(curr_dir, 'datasets/s9-t_0.4-c_1')\n",
        "    set10_dir = os.path.join(curr_dir, 'datasets/s10-t_0.8-c_0.3')\n",
        "    set11_dir = os.path.join(curr_dir, 'datasets/s11-t_0.8-c_0.45')\n",
        "    set12_dir = os.path.join(curr_dir, 'datasets/s12-t_0.8-c_1')\n",
        "    set13_dir = os.path.join(curr_dir, 'datasets/s13-t_1.6-c_0.3')\n",
        "    set14_dir = os.path.join(curr_dir, 'datasets/s14-t_1.6-c_0.45')\n",
        "    set15_dir = os.path.join(curr_dir, 'datasets/s15-t_1.6-c_1')\n",
        "    set16_dir = os.path.join(curr_dir, 'datasets/s16-t_3.2-c_0.3')\n",
        "    set17_dir = os.path.join(curr_dir, 'datasets/s17-t_3.2-c_0.45')\n",
        "    set18_dir = os.path.join(curr_dir, 'datasets/s18-t_3.2-c_1')\n",
        "    set1 = image_dataset_from_directory(set1_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE) #3000 images 2 classes\n",
        "    set2 = image_dataset_from_directory(set2_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set3 = image_dataset_from_directory(set3_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set4 = image_dataset_from_directory(set4_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set5 = image_dataset_from_directory(set5_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set6 = image_dataset_from_directory(set6_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set7 = image_dataset_from_directory(set7_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE) #3000 images 2 classes\n",
        "    set8 = image_dataset_from_directory(set8_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set9 = image_dataset_from_directory(set9_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set10 = image_dataset_from_directory(set10_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set11 = image_dataset_from_directory(set11_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set12 = image_dataset_from_directory(set12_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set13 = image_dataset_from_directory(set13_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE) #3000 images 2 classes\n",
        "    set14 = image_dataset_from_directory(set14_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set15 = image_dataset_from_directory(set15_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set16 = image_dataset_from_directory(set16_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set17 = image_dataset_from_directory(set17_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set18 = image_dataset_from_directory(set18_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set2 = set2.prefetch(buffer_size=AUTOTUNE) \n",
        "    set1 = set1.prefetch(buffer_size=AUTOTUNE) \n",
        "    set3 = set3.prefetch(buffer_size=AUTOTUNE) \n",
        "    set4 = set4.prefetch(buffer_size=AUTOTUNE) \n",
        "    set5 = set5.prefetch(buffer_size=AUTOTUNE) \n",
        "    set6 = set6.prefetch(buffer_size=AUTOTUNE) \n",
        "    set7 = set7.prefetch(buffer_size=AUTOTUNE) \n",
        "    set8 = set8.prefetch(buffer_size=AUTOTUNE) \n",
        "    set9 = set9.prefetch(buffer_size=AUTOTUNE) \n",
        "    set10 = set10.prefetch(buffer_size=AUTOTUNE)\n",
        "    set11 = set11.prefetch(buffer_size=AUTOTUNE)\n",
        "    set12 = set12.prefetch(buffer_size=AUTOTUNE)\n",
        "    set13 = set13.prefetch(buffer_size=AUTOTUNE)\n",
        "    set14 = set14.prefetch(buffer_size=AUTOTUNE)\n",
        "    set15 = set15.prefetch(buffer_size=AUTOTUNE)\n",
        "    set16 = set16.prefetch(buffer_size=AUTOTUNE)\n",
        "    set17 = set17.prefetch(buffer_size=AUTOTUNE)\n",
        "    set18 = set18.prefetch(buffer_size=AUTOTUNE)\n",
        "    return set1,set2,set3,set4,set5,set6,set7,set8,set9,set10,set11,set12,set13,set14,set15,set16,set17,set18\n",
        "\n",
        "set1,set2,set3,set4,set5,set6,set7,set8,set9,set10,set11,set12,set13,set14,set15,set16,set17,set18 = model2_init_sets(BATCH_SIZE, IMG_SIZE, AUTOTUNE)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E54Mm44j35qn"
      },
      "source": [
        "### 3: Test different datasets\n",
        "Setup and run the test to calculate both Tf's accuracy and my naive calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkD8QOuqB93g"
      },
      "source": [
        "def test_dataset(current_set):\n",
        "  all_acc=tf.zeros([], tf.float64) #initialize array to hold all accuracy indicators (single element)\n",
        "  loss, acc = model.evaluate(current_set) #now test the model's performance on the test set\n",
        "  for image_batch, label_batch in current_set.as_numpy_iterator():\n",
        "      predictions = model.predict_on_batch(image_batch).flatten() #run batch through model and return logits\n",
        "      predictions = tf.nn.sigmoid(predictions) #apply sigmoid activation function to transform logits to [0,1]\n",
        "      predictions = tf.where(predictions < 0.5, 0, 1) #round down or up accordingly since it's a binary classifier\n",
        "      accuracy = tf.where(tf.equal(predictions,label_batch),1,0) #correct is 1 and incorrect is 0\n",
        "      all_acc = tf.experimental.numpy.append(all_acc, accuracy)\n",
        "  all_acc = all_acc[1:]  #drop first placeholder element\n",
        "  avg_acc = tf.reduce_mean(all_acc)\n",
        "  print('My Accuracy:', avg_acc.numpy()) \n",
        "  print('Tf Accuracy:', acc) \n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAX3FAtK4UeJ",
        "outputId": "7b2c6e98-d46a-446c-c7ba-8e4e422a4045",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_dataset(set1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 16s 451ms/step - loss: 0.6922 - accuracy: 0.5030\n",
            "My Accuracy: 0.543\n",
            "Tf Accuracy: 0.503000020980835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAodEKlZ43Pj"
      },
      "source": [
        "My accuracy yielded a 54.3% accuracy while Tensorflow's yielded 50.30%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSODie-t4xsu",
        "outputId": "329a8276-3050-4697-b4f0-2b0d2f946916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_dataset(set4)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 15s 449ms/step - loss: 0.6854 - accuracy: 0.5040\n",
            "My Accuracy: 0.558\n",
            "Tf Accuracy: 0.5040000081062317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAezusdk5EbK"
      },
      "source": [
        "My accuracy yielded a 54.3% accuracy while Tensorflow's yielded 50.30%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_bgBCBz5F2P",
        "outputId": "94b66b0f-d232-4f0e-f75e-7b5b3ae0bf87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_dataset(set10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 14s 439ms/step - loss: 0.6817 - accuracy: 0.5040\n",
            "My Accuracy: 0.552\n",
            "Tf Accuracy: 0.5040000081062317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOYvmJPi5Lgd"
      },
      "source": [
        "My accuracy yielded a 55.2% accuracy while Tensorflow's yielded 50.40%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVaDM4XL5MFL",
        "outputId": "4026427a-8596-4087-b6a0-db89c7c5a769",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_dataset(set15)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 14s 439ms/step - loss: 0.1042 - accuracy: 0.9660\n",
            "My Accuracy: 0.987\n",
            "Tf Accuracy: 0.9660000205039978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0lAZ3K_5i8k"
      },
      "source": [
        "My accuracy yielded a 98.7% accuracy while Tensorflow's yielded 96.6%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiT1eVSh5lBQ",
        "outputId": "6e9c7c4f-5f8d-4534-b45d-27ceac15a306",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_dataset(set16)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 14s 434ms/step - loss: 0.6551 - accuracy: 0.5200\n",
            "My Accuracy: 0.648\n",
            "Tf Accuracy: 0.5199999809265137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl3snT2d5s4i"
      },
      "source": [
        "My accuracy yielded a 64.8% accuracy while Tensorflow's yielded ~52%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOc2iZob5vlC",
        "outputId": "918fa484-d34a-4dcd-cc26-ef98e9ea81ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_dataset(set17)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 15s 451ms/step - loss: 0.5398 - accuracy: 0.6750\n",
            "My Accuracy: 0.832\n",
            "Tf Accuracy: 0.675000011920929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4qiHWRv57nC"
      },
      "source": [
        "My accuracy yielded a 83.2% accuracy while Tensorflow's yielded 67.5%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjKHej5f59Tl",
        "outputId": "d030c93c-15b0-4fb3-b719-0e139c990edc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_dataset(set18)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 15s 457ms/step - loss: 0.0091 - accuracy: 1.0000\n",
            "My Accuracy: 1.0\n",
            "Tf Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7EYd0zK6JXd"
      },
      "source": [
        "My accuracy yielded 100% accuracy while Tensorflow's yielded 100%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sRRB5md6WYS"
      },
      "source": [
        "Dataset 17 showed the biggest disparity and dataset 18 was the only set that showed the same accuracy in both calculations"
      ]
    }
  ]
}