{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "step2-tilt_search",
      "provenance": [],
      "authorship_tag": "ABX9TyNGJ5MMw35LIcYI29Tq7T/O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thor4/neuralnets/blob/master/projects/1-CNN/step2-tilt_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHcTgWXkB-4b"
      },
      "source": [
        "# Tilt search\n",
        "--- "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will test a range of 25 tilts evenly spaced from [0.05,2] at .45 contrast in each trained network and pick those that produce ~60%, ~70%, and ~80% correct to match human behavioral performance. Then, we will test each qualifying tilt with contrasts of 0.3, .45, and 1."
      ],
      "metadata": {
        "id": "BjtepbQ01mJ4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6D3EmbiHej8",
        "outputId": "20c1e382-bf1b-4c9b-cd17-a98d18d67648",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar 16 13:28:37 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgnGnDu-HfF8",
        "outputId": "5bfe037d-79da-4d6c-c24c-463cdc86b7fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKux6e-YPew0"
      },
      "source": [
        "## 1: Setup the model\n",
        "Choose the model we will be using to search for qualifying tilts. The models were created using the `step1` Jupyter notebook. \n",
        "Run the cell to download a zip file from OSF then extract its contents into the newly created directory.\n",
        "\n",
        "vanilla gabor model: `content/van_gabor/`\n",
        "\n",
        "cifar10 gabor model: `content/cifar10_gabors/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpRtxiFmDo7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "22285455-4069-4909-9c14-a952c5005a02"
      },
      "source": [
        "# @title Download vanilla Gabor model\n",
        "\n",
        "import requests, os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "print(\"Start downloading and unzipping `vanilla model trained on Gabors`...\")\n",
        "name = 'van_gabor_model.zip'\n",
        "fname = f\"{name}.zip\"\n",
        "url = f\"https://osf.io/3wqsf/download\" #osf share link\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "with open(fname, 'wb') as fh:\n",
        "  fh.write(r.content) #download file\n",
        "\n",
        "with ZipFile(fname, 'r') as zfile:\n",
        "  zfile.extractall() #extract contents\n",
        "\n",
        "if os.path.exists(fname):\n",
        "  os.remove(fname) #delete zip file\n",
        "else:\n",
        "  print(f\"The file {fname} does not exist\")\n",
        "\n",
        "print(\"Download completed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading and unzipping `vanilla model trained on Gabors`...\n",
            "Download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download cifar10 Gabor model\n",
        "\n",
        "import requests, os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "print(\"Start downloading and unzipping cifar10 model fine-tuned on Gabors`...\")\n",
        "name = 'cifar10_gabors_model.zip'\n",
        "fname = f\"{name}.zip\"\n",
        "url = f\"https://osf.io/x8uve/download\" #osf share link\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "with open(fname, 'wb') as fh:\n",
        "  fh.write(r.content) #download file\n",
        "\n",
        "with ZipFile(fname, 'r') as zfile:\n",
        "  zfile.extractall() #extract contents\n",
        "\n",
        "if os.path.exists(fname):\n",
        "  os.remove(fname) #delete zip file\n",
        "else:\n",
        "  print(f\"The file {fname} does not exist\")\n",
        "\n",
        "print(\"Download completed.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JTZOUSGUx80r",
        "outputId": "7e022751-1550-45ff-b8bf-b5417cf78da4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading and unzipping cifar10 model fine-tuned on Gabors`...\n",
            "Download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOM5VyU8x80i"
      },
      "source": [
        "#### Load the model\n",
        "Next, we load the model using Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RVZI9Dexw2N",
        "outputId": "a069d161-b391-43e6-9aa3-cef9542c2db5"
      },
      "source": [
        "import tensorflow as tf \n",
        "import pandas as pd\n",
        "tf.random.set_seed(42) #set random seed for reproducibility\n",
        "#model = tf.keras.models.load_model('van_gabor') \n",
        "model = tf.keras.models.load_model('cifar10_gabors') \n",
        "model.summary() #verify architecture"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 160, 160, 3)]     0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 80)                2193440   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 81        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,193,521\n",
            "Trainable params: 81\n",
            "Non-trainable params: 2,193,440\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZXJPKMezPAP"
      },
      "source": [
        "## 2: Download & load datasets to test model with\n",
        "Download the test datasets from OSF and extract the contents into the newly created directory: `content/datasets/`.\n",
        "The datasets were created in MATLAB using the script `gen_gabor-range_rand-test_sets.m`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJiTI49Sz6yw",
        "outputId": "8a4607cb-e01f-4070-87fb-24f32977ca0c"
      },
      "source": [
        "# @title Download datasets\n",
        "\n",
        "print(\"Start downloading and unzipping 25 test datasets...\")\n",
        "name = 'tilt_0_05-2_contrast_0_45'\n",
        "fname = f\"{name}.zip\"\n",
        "url = f\"https://osf.io/yavbx/download\" #osf share link\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "with open(fname, 'wb') as fh:\n",
        "  fh.write(r.content) #download file\n",
        "\n",
        "with ZipFile(fname, 'r') as zfile:\n",
        "  zfile.extractall(\"datasets\") #extract contents\n",
        "\n",
        "if os.path.exists(fname):\n",
        "  os.remove(fname) #delete zip file\n",
        "else:\n",
        "  print(f\"The file {fname} does not exist\")\n",
        "\n",
        "print(\"Download completed.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading and unzipping 25 test datasets...\n",
            "Download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4rHfgAq0nbi"
      },
      "source": [
        "Load all 25 sets and use prefetch to streamline image loading. Contrast is 0.45 and the 25 tilts are: \n",
        "\n",
        "[0.0500, 0.1313, 0.2125, 0.2938, 0.3750, 0.4562, 0.5375, 0.6188, 0.7000, 0.7813, 0.8625, 0.9438, 1.0250, 1.1062, 1.1875, 1.2687, 1.3500, 1.4312, 1.5125, 1.5938, 1.6750, 1.7563, 1.8375, 1.9187, 2.0000]\n",
        "\n",
        "Make sure to specify `color_mode='grayscale` for vanilla gabor model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVkzI7QC2gFL",
        "cellView": "form",
        "outputId": "80679af1-3913-4fbf-ecb1-94c32c704a5c"
      },
      "source": [
        "# @title Load datasets into tensorflow\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "BATCH_SIZE = 32 \n",
        "IMG_SIZE = (160, 160) #forces a resize from 169x169 \n",
        "AUTOTUNE = tf.data.AUTOTUNE #prompts the tf.data runtime to tune the value dynamically at runtime\n",
        "def model_init_sets(BATCH_SIZE, IMG_SIZE, AUTOTUNE):\n",
        "    curr_dir = os.getcwd() \n",
        "    set1_dir = os.path.join(curr_dir, 'datasets/s1')\n",
        "    set2_dir = os.path.join(curr_dir, 'datasets/s2')\n",
        "    set3_dir = os.path.join(curr_dir, 'datasets/s3')\n",
        "    set4_dir = os.path.join(curr_dir, 'datasets/s4')\n",
        "    set5_dir = os.path.join(curr_dir, 'datasets/s5')\n",
        "    set6_dir = os.path.join(curr_dir, 'datasets/s6')\n",
        "    set7_dir = os.path.join(curr_dir, 'datasets/s7')\n",
        "    set8_dir = os.path.join(curr_dir, 'datasets/s8')\n",
        "    set9_dir = os.path.join(curr_dir, 'datasets/s9')\n",
        "    set10_dir = os.path.join(curr_dir, 'datasets/s10')\n",
        "    set11_dir = os.path.join(curr_dir, 'datasets/s11')\n",
        "    set12_dir = os.path.join(curr_dir, 'datasets/s12')\n",
        "    set13_dir = os.path.join(curr_dir, 'datasets/s13')\n",
        "    set14_dir = os.path.join(curr_dir, 'datasets/s14')\n",
        "    set15_dir = os.path.join(curr_dir, 'datasets/s15')\n",
        "    set16_dir = os.path.join(curr_dir, 'datasets/s16')\n",
        "    set17_dir = os.path.join(curr_dir, 'datasets/s17')\n",
        "    set18_dir = os.path.join(curr_dir, 'datasets/s18')\n",
        "    set19_dir = os.path.join(curr_dir, 'datasets/s19')\n",
        "    set20_dir = os.path.join(curr_dir, 'datasets/s20')\n",
        "    set21_dir = os.path.join(curr_dir, 'datasets/s21')\n",
        "    set22_dir = os.path.join(curr_dir, 'datasets/s22')\n",
        "    set23_dir = os.path.join(curr_dir, 'datasets/s23')\n",
        "    set24_dir = os.path.join(curr_dir, 'datasets/s24')\n",
        "    set25_dir = os.path.join(curr_dir, 'datasets/s25')\n",
        "    set1 = image_dataset_from_directory(set1_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE) #2000 images 2 classes\n",
        "    set2 = image_dataset_from_directory(set2_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set3 = image_dataset_from_directory(set3_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set4 = image_dataset_from_directory(set4_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set5 = image_dataset_from_directory(set5_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set6 = image_dataset_from_directory(set6_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set7 = image_dataset_from_directory(set7_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE) \n",
        "    set8 = image_dataset_from_directory(set8_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set9 = image_dataset_from_directory(set9_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set10 = image_dataset_from_directory(set10_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set11 = image_dataset_from_directory(set11_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set12 = image_dataset_from_directory(set12_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set13 = image_dataset_from_directory(set13_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE) \n",
        "    set14 = image_dataset_from_directory(set14_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set15 = image_dataset_from_directory(set15_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set16 = image_dataset_from_directory(set16_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set17 = image_dataset_from_directory(set17_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set18 = image_dataset_from_directory(set18_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set19 = image_dataset_from_directory(set19_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set20 = image_dataset_from_directory(set20_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set21 = image_dataset_from_directory(set21_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE) \n",
        "    set22 = image_dataset_from_directory(set22_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set23 = image_dataset_from_directory(set23_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set24 = image_dataset_from_directory(set24_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    set25 = image_dataset_from_directory(set25_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "    class_names = set1.class_names #extract class names loading function inferred from subdir's\n",
        "    set1 = set1.prefetch(buffer_size=AUTOTUNE) \n",
        "    set2 = set2.prefetch(buffer_size=AUTOTUNE) \n",
        "    set3 = set3.prefetch(buffer_size=AUTOTUNE) \n",
        "    set4 = set4.prefetch(buffer_size=AUTOTUNE) \n",
        "    set5 = set5.prefetch(buffer_size=AUTOTUNE) \n",
        "    set6 = set6.prefetch(buffer_size=AUTOTUNE) \n",
        "    set7 = set7.prefetch(buffer_size=AUTOTUNE) \n",
        "    set8 = set8.prefetch(buffer_size=AUTOTUNE) \n",
        "    set9 = set9.prefetch(buffer_size=AUTOTUNE) \n",
        "    set10 = set10.prefetch(buffer_size=AUTOTUNE)\n",
        "    set11 = set11.prefetch(buffer_size=AUTOTUNE)\n",
        "    set12 = set12.prefetch(buffer_size=AUTOTUNE)\n",
        "    set13 = set13.prefetch(buffer_size=AUTOTUNE)\n",
        "    set14 = set14.prefetch(buffer_size=AUTOTUNE)\n",
        "    set15 = set15.prefetch(buffer_size=AUTOTUNE)\n",
        "    set16 = set16.prefetch(buffer_size=AUTOTUNE)\n",
        "    set17 = set17.prefetch(buffer_size=AUTOTUNE)\n",
        "    set18 = set18.prefetch(buffer_size=AUTOTUNE)\n",
        "    set19 = set19.prefetch(buffer_size=AUTOTUNE)\n",
        "    set20 = set20.prefetch(buffer_size=AUTOTUNE)\n",
        "    set21 = set21.prefetch(buffer_size=AUTOTUNE)\n",
        "    set22 = set22.prefetch(buffer_size=AUTOTUNE)\n",
        "    set23 = set23.prefetch(buffer_size=AUTOTUNE)\n",
        "    set24 = set24.prefetch(buffer_size=AUTOTUNE)\n",
        "    set25 = set25.prefetch(buffer_size=AUTOTUNE)\n",
        "    return set1,set2,set3,set4,set5,set6,set7,set8,set9,set10,set11,set12,set13,set14,set15,set16,set17,set18,set19,set20,set21,set22,set23,set24,set25,class_names\n",
        "\n",
        "set1,set2,set3,set4,set5,set6,set7,set8,set9,set10,set11,set12,set13,set14,set15,set16,set17,set18,set19,set20,set21,set22,set23,set24,set25,class_names = model_init_sets(BATCH_SIZE, IMG_SIZE, AUTOTUNE)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2FohnCiVibU"
      },
      "source": [
        "##3: Generate accuracy\n",
        "First, we can define a function for generating accuracy from a dataset processed by a particular model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1i50PBHVgzI"
      },
      "source": [
        "from scipy.special import expit #import sigmoid func\n",
        "import numpy as np\n",
        "def get_acc(dataset, model):\n",
        "    all_pred=tf.zeros([], tf.float64) #initialize array to hold all prediction logits (single element)\n",
        "    all_labels=tf.zeros([], tf.float64) #initialize array to hold all actual labels (single element)\n",
        "    for image_batch, label_batch in dataset.as_numpy_iterator():\n",
        "        predictions = model.predict_on_batch(image_batch).flatten() #run batch through model and return logits\n",
        "        all_pred = tf.experimental.numpy.append(all_pred, predictions)\n",
        "        all_labels = tf.experimental.numpy.append(all_labels, label_batch)\n",
        "    #tf.size(all_pred) #1335 elements, 1334 images + 1 placeholder 0 at beginning\n",
        "    all_pred = all_pred[1:] #remove placeholder 0 at beginning\n",
        "    all_labels = all_labels[1:]\n",
        "    all_pred_sig = expit(all_pred) #sigmoid-transform the logits\n",
        "    all_pred_round = np.where((all_pred_sig < 0.5), 0, 1) #replace predictions with 0 or 1\n",
        "    all_acc = np.where((all_pred_round == all_labels), 1, 0) #decide whether pred = label\n",
        "    return all_pred,all_labels,all_acc"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UUQRG-9EKgI"
      },
      "source": [
        "Let's test it on the first dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8asmKVbtWqYE"
      },
      "source": [
        "all_pred, all_labels, all_acc = get_acc(set25,model)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8HwdbIbsYLo",
        "outputId": "b184fb40-e638-4aa4-8d62-431d5bb24e47"
      },
      "source": [
        "print(all_pred.numpy()[1000:1055]) #pick 55 logits to display (neg=class 0, pos=1)\n",
        "print(all_labels.numpy()[1000:1055]) #associated labels\n",
        "print(all_acc[1000:1055]) #associated accuracies"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.16947237 -1.0594064  -1.57268822  0.00923482  0.51718032  0.34807172\n",
            "  0.06087866 -0.71026027  0.55679929 -0.12992701 -0.29608926 -0.72555411\n",
            " -0.54416311  0.0266954   0.55864751 -1.11908877 -1.18175924  0.25399557\n",
            "  0.15574208 -0.818174   -0.89085853 -0.43859634 -0.45558056 -0.46937975\n",
            " -0.50722444 -1.36867917 -0.0542849   0.71397388  0.33489981 -0.55787766\n",
            "  0.54769886  0.42610732  1.40527582  0.28617159 -0.90042794 -0.22953853\n",
            " -0.26802024  0.17736092  1.28606641  0.04581586 -0.08283338 -1.63446295\n",
            "  0.3079932   0.99803555  0.21762887 -0.96928942 -1.2461015  -0.14247736\n",
            " -0.71445405  0.82850075  0.74063528 -0.86678445  0.59412539 -0.67658651\n",
            " -1.17064202]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1\n",
            " 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These look like the function is working correctly."
      ],
      "metadata": {
        "id": "olhaj4og0CHA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2r3dNtSmNTI",
        "outputId": "8082c004-2348-4946-f5e0-e293e39c46f4"
      },
      "source": [
        "all_acc.mean() "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.70075"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks accurate. The negative logits were class 0 and positive class 1. All were supposed to be class 0. Hence, the accuracy. \n",
        "\n",
        "Now, we can process all datasets and save into a dataframe, `df`."
      ],
      "metadata": {
        "id": "mK6GsV0-QzC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['Tilts','Accuracy']) #initialize dataframe\n",
        "all_sets = [set1,set2,set3,set4,set5,set6,set7,set8,set9,set10,set11,set12,set13,set14,set15,set16,set17,set18,set19,set20,set21,set22,set23,set24,set25]\n",
        "tilts = [0.0500,0.1313,0.2125,0.2938,0.3750,0.4562,0.5375,0.6188,0.7000,0.7813,0.8625,0.9438,1.0250,1.1062,1.1875,1.2687,1.3500,1.4312,1.5125,1.5938,1.6750,1.7563,1.8375,1.9187,2.0000]\n",
        "idx=0;\n",
        "for idx, dataset in enumerate(all_sets): #run for all sets:\n",
        "    all_pred, all_labels, all_acc = get_acc(dataset, model)\n",
        "    df = pd.concat([df, pd.DataFrame({'Tilts':[tilts[idx]],\n",
        "                                      'Accuracy':[all_acc.mean()]})], \n",
        "                   axis=0, ignore_index=True) #append tilts & accuracies to dataframe"
      ],
      "metadata": {
        "id": "1cb-9LSxOtVg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "_N0ipL-A_7_I",
        "outputId": "05f7e665-b724-4f17-a470-74dff528365e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Tilts  Accuracy\n",
              "0   0.0500   0.49825\n",
              "1   0.1313   0.52875\n",
              "2   0.2125   0.51300\n",
              "3   0.2938   0.54275\n",
              "4   0.3750   0.53650\n",
              "5   0.4562   0.53950\n",
              "6   0.5375   0.54875\n",
              "7   0.6188   0.55900\n",
              "8   0.7000   0.58325\n",
              "9   0.7813   0.58150\n",
              "10  0.8625   0.58075\n",
              "11  0.9438   0.59500\n",
              "12  1.0250   0.60100\n",
              "13  1.1062   0.61000\n",
              "14  1.1875   0.62975\n",
              "15  1.2687   0.63675\n",
              "16  1.3500   0.63725\n",
              "17  1.4312   0.65300\n",
              "18  1.5125   0.64100\n",
              "19  1.5938   0.65500\n",
              "20  1.6750   0.65800\n",
              "21  1.7563   0.65975\n",
              "22  1.8375   0.67850\n",
              "23  1.9187   0.67375\n",
              "24  2.0000   0.70075"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb840520-cbaa-4b27-8806-e3f42fb51166\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tilts</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0500</td>\n",
              "      <td>0.49825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.1313</td>\n",
              "      <td>0.52875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.2125</td>\n",
              "      <td>0.51300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.2938</td>\n",
              "      <td>0.54275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.3750</td>\n",
              "      <td>0.53650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.4562</td>\n",
              "      <td>0.53950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.5375</td>\n",
              "      <td>0.54875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.6188</td>\n",
              "      <td>0.55900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.7000</td>\n",
              "      <td>0.58325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.7813</td>\n",
              "      <td>0.58150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.8625</td>\n",
              "      <td>0.58075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.9438</td>\n",
              "      <td>0.59500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.0250</td>\n",
              "      <td>0.60100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.1062</td>\n",
              "      <td>0.61000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.1875</td>\n",
              "      <td>0.62975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.2687</td>\n",
              "      <td>0.63675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.3500</td>\n",
              "      <td>0.63725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.4312</td>\n",
              "      <td>0.65300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.5125</td>\n",
              "      <td>0.64100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.5938</td>\n",
              "      <td>0.65500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.6750</td>\n",
              "      <td>0.65800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.7563</td>\n",
              "      <td>0.65975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.8375</td>\n",
              "      <td>0.67850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.9187</td>\n",
              "      <td>0.67375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2.0000</td>\n",
              "      <td>0.70075</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb840520-cbaa-4b27-8806-e3f42fb51166')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bb840520-cbaa-4b27-8806-e3f42fb51166 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bb840520-cbaa-4b27-8806-e3f42fb51166');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wc47oZY-QM_"
      },
      "source": [
        "Finally, we can save the dataframe as an excel file to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UVlcQ7_tsD-"
      },
      "source": [
        "#logits_excel_filepath = os.path.join(os.getcwd(), 'tilts_acc-van_gabor.xlsx') #prep path to save to\n",
        "logits_excel_filepath = os.path.join(os.getcwd(), 'tilts_acc-cifar10_gabor.xlsx') #prep path to save to\n",
        "\n",
        "df.to_excel(logits_excel_filepath, index=False) #save to disk"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}