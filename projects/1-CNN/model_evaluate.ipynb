{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_evaluate.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPHsTp2sNLH52WPjdaw0Ee4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thor4/neuralnets/blob/master/projects/1-CNN/model_evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHcTgWXkB-4b"
      },
      "source": [
        "## Steps to reproduce `model.evaluate()` error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63uVilpRCI6P"
      },
      "source": [
        "### 1: Load the model\n",
        "\n",
        "This model was created using Tensorflow's transfer learning tutorial. MobileNetV2 was used as the convolutional base. Gabors in two classes representing tilts from 45 degrees in the clockwise direction and tilts in the counterclockwise direction were used to train the final classification layer. Finally, the model was fine-tuned by training the weights for layers 101 onwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpRtxiFmDo7x"
      },
      "source": [
        "import requests, os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "print(\"Start downloading and unzipping `Model 2 Training` dataset...\")\n",
        "name = '18kim_range_ft'\n",
        "fname = f\"{name}.zip\"\n",
        "url = f\"https://osf.io/vbwsk/download\" #osf share link\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "with open(fname, 'wb') as fh:\n",
        "  fh.write(r.content) #download file\n",
        "\n",
        "with ZipFile(fname, 'r') as zfile:\n",
        "  zfile.extractall(f\"./{name}\") #extract contents\n",
        "\n",
        "if os.path.exists(fname):\n",
        "  os.remove(fname) #delete zip file\n",
        "else:\n",
        "  print(f\"The file {fname} does not exist\")\n",
        "\n",
        "print(\"Download completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkD8QOuqB93g"
      },
      "source": [
        "current_set = set17 #define set to process.\n",
        "all_acc=tf.zeros([], tf.float64) #initialize array to hold all accuracy indicators (single element)\n",
        "loss, acc = model.evaluate(current_set) #now test the model's performance on the test set\n",
        "for image_batch, label_batch in current_set.as_numpy_iterator():\n",
        "    predictions = model.predict_on_batch(image_batch).flatten() #run batch through model and return logits\n",
        "    predictions = tf.nn.sigmoid(predictions) #apply sigmoid activation function to transform logits to [0,1]\n",
        "    predictions = tf.where(predictions < 0.5, 0, 1) #round down or up accordingly since it's a binary classifier\n",
        "    accuracy = tf.where(tf.equal(predictions,label_batch),1,0) #correct is 1 and incorrect is 0\n",
        "    all_acc = tf.experimental.numpy.append(all_acc, accuracy)\n",
        "all_acc = all_acc[1:]  #drop first placeholder element\n",
        "avg_acc = tf.reduce_mean(all_acc)\n",
        "print('My Accuracy:', avg_acc.numpy()) \n",
        "print('Tf Accuracy:', acc) \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}