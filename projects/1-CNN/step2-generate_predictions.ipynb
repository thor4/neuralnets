{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "step2-generation_predictions",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP3MyXTW6z4efPkLvx05pqX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thor4/neuralnets/blob/master/projects/1-CNN/step2-generate_predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHcTgWXkB-4b"
      },
      "source": [
        "# Generate predictions from model\n",
        "--- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKux6e-YPew0"
      },
      "source": [
        "## 1: Setup the model\n",
        "This model was created using the `step1_train_vanilla_CNN_v2.ipynb` Jupyter notebook. \n",
        "Run the cell to download a zip file from OSF then extract its contents into the newly created directory: `content/18kim_range_vanilla/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpRtxiFmDo7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "b04a42a5-de6b-4ee5-8704-aab2ad399ae1"
      },
      "source": [
        "# @title Download model\n",
        "\n",
        "import requests, os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "print(\"Start downloading and unzipping `Range model`...\")\n",
        "name = '18kim_range_vanilla'\n",
        "fname = f\"{name}.zip\"\n",
        "url = f\"https://osf.io/tycf6/download\" #osf share link\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "with open(fname, 'wb') as fh:\n",
        "  fh.write(r.content) #download file\n",
        "\n",
        "with ZipFile(fname, 'r') as zfile:\n",
        "  zfile.extractall() #extract contents\n",
        "\n",
        "if os.path.exists(fname):\n",
        "  os.remove(fname) #delete zip file\n",
        "else:\n",
        "  print(f\"The file {fname} does not exist\")\n",
        "\n",
        "print(\"Download completed.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading and unzipping `Range model`...\n",
            "Download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOM5VyU8x80i"
      },
      "source": [
        "#### Load the model\n",
        "Next, we load the model using Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RVZI9Dexw2N",
        "outputId": "08e322f3-ee58-41a4-b9e6-9ffadd5bbc23"
      },
      "source": [
        "import tensorflow as tf \n",
        "model = tf.keras.models.load_model('18kim_range_vanilla') \n",
        "model.summary() #verify architecture"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_1 (Rescaling)      (None, 160, 160, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 158, 158, 160)     1600      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 79, 79, 160)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 77, 77, 80)        115280    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 38, 38, 80)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 36, 36, 40)        28840     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 51840)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 40)                2073640   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 2,219,401\n",
            "Trainable params: 2,219,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZXJPKMezPAP"
      },
      "source": [
        "## 2: Download & load datasets to test model with\n",
        "Download the test datasets from OSF and extract the contents into the newly created directory: `content/datasets/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "CJiTI49Sz6yw",
        "outputId": "51c07d1c-38dc-40ec-efa5-7339d95e344d"
      },
      "source": [
        "# @title Download datasets\n",
        "\n",
        "print(\"Start downloading and unzipping `18 test datasets`...\")\n",
        "name = 'model2_dset1-18_cond'\n",
        "fname = f\"{name}.zip\"\n",
        "url = f\"https://osf.io/jkryf/download\" #osf share link\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "with open(fname, 'wb') as fh:\n",
        "  fh.write(r.content) #download file\n",
        "\n",
        "with ZipFile(fname, 'r') as zfile:\n",
        "  zfile.extractall(\"datasets\") #extract contents\n",
        "\n",
        "if os.path.exists(fname):\n",
        "  os.remove(fname) #delete zip file\n",
        "else:\n",
        "  print(f\"The file {fname} does not exist\")\n",
        "\n",
        "print(\"Download completed.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading and unzipping `18 test datasets`...\n",
            "Download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4rHfgAq0nbi"
      },
      "source": [
        "Load all 18 sets and use prefetch to streamline image loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVkzI7QC2gFL",
        "cellView": "form",
        "outputId": "66b05fa9-4643-405f-c6b4-d8d3c9400109"
      },
      "source": [
        "# @title Load datasets into tensorflow\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "BATCH_SIZE = 32 \n",
        "IMG_SIZE = (160, 160) #forces a resize from 170x170 since MobileNetV2 has weights only for certain sizes\n",
        "AUTOTUNE = tf.data.AUTOTUNE #prompts the tf.data runtime to tune the value dynamically at runtime\n",
        "def model2_init_sets(BATCH_SIZE, IMG_SIZE, AUTOTUNE):\n",
        "    curr_dir = os.getcwd() \n",
        "    set1_dir = os.path.join(curr_dir, 'datasets/s1-t_0.1-c_0.3')\n",
        "    set2_dir = os.path.join(curr_dir, 'datasets/s2-t_0.1-c_0.45')\n",
        "    set3_dir = os.path.join(curr_dir, 'datasets/s3-t_0.1-c_1')\n",
        "    set4_dir = os.path.join(curr_dir, 'datasets/s4-t_0.2-c_0.3')\n",
        "    set5_dir = os.path.join(curr_dir, 'datasets/s5-t_0.2-c_0.45')\n",
        "    set6_dir = os.path.join(curr_dir, 'datasets/s6-t_0.2-c_1')\n",
        "    set7_dir = os.path.join(curr_dir, 'datasets/s7-t_0.4-c_0.3')\n",
        "    set8_dir = os.path.join(curr_dir, 'datasets/s8-t_0.4-c_0.45')\n",
        "    set9_dir = os.path.join(curr_dir, 'datasets/s9-t_0.4-c_1')\n",
        "    set10_dir = os.path.join(curr_dir, 'datasets/s10-t_0.8-c_0.3')\n",
        "    set11_dir = os.path.join(curr_dir, 'datasets/s11-t_0.8-c_0.45')\n",
        "    set12_dir = os.path.join(curr_dir, 'datasets/s12-t_0.8-c_1')\n",
        "    set13_dir = os.path.join(curr_dir, 'datasets/s13-t_1.6-c_0.3')\n",
        "    set14_dir = os.path.join(curr_dir, 'datasets/s14-t_1.6-c_0.45')\n",
        "    set15_dir = os.path.join(curr_dir, 'datasets/s15-t_1.6-c_1')\n",
        "    set16_dir = os.path.join(curr_dir, 'datasets/s16-t_3.2-c_0.3')\n",
        "    set17_dir = os.path.join(curr_dir, 'datasets/s17-t_3.2-c_0.45')\n",
        "    set18_dir = os.path.join(curr_dir, 'datasets/s18-t_3.2-c_1')\n",
        "    set1 = image_dataset_from_directory(set1_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale') #3000 images 2 classes\n",
        "    set2 = image_dataset_from_directory(set2_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set3 = image_dataset_from_directory(set3_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set4 = image_dataset_from_directory(set4_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set5 = image_dataset_from_directory(set5_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set6 = image_dataset_from_directory(set6_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set7 = image_dataset_from_directory(set7_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale') #3000 images 2 classes\n",
        "    set8 = image_dataset_from_directory(set8_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set9 = image_dataset_from_directory(set9_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set10 = image_dataset_from_directory(set10_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set11 = image_dataset_from_directory(set11_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set12 = image_dataset_from_directory(set12_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set13 = image_dataset_from_directory(set13_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale') #3000 images 2 classes\n",
        "    set14 = image_dataset_from_directory(set14_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set15 = image_dataset_from_directory(set15_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set16 = image_dataset_from_directory(set16_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set17 = image_dataset_from_directory(set17_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set18 = image_dataset_from_directory(set18_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set2 = set2.prefetch(buffer_size=AUTOTUNE) \n",
        "    set1 = set1.prefetch(buffer_size=AUTOTUNE) \n",
        "    set3 = set3.prefetch(buffer_size=AUTOTUNE) \n",
        "    set4 = set4.prefetch(buffer_size=AUTOTUNE) \n",
        "    set5 = set5.prefetch(buffer_size=AUTOTUNE) \n",
        "    set6 = set6.prefetch(buffer_size=AUTOTUNE) \n",
        "    set7 = set7.prefetch(buffer_size=AUTOTUNE) \n",
        "    set8 = set8.prefetch(buffer_size=AUTOTUNE) \n",
        "    set9 = set9.prefetch(buffer_size=AUTOTUNE) \n",
        "    set10 = set10.prefetch(buffer_size=AUTOTUNE)\n",
        "    set11 = set11.prefetch(buffer_size=AUTOTUNE)\n",
        "    set12 = set12.prefetch(buffer_size=AUTOTUNE)\n",
        "    set13 = set13.prefetch(buffer_size=AUTOTUNE)\n",
        "    set14 = set14.prefetch(buffer_size=AUTOTUNE)\n",
        "    set15 = set15.prefetch(buffer_size=AUTOTUNE)\n",
        "    set16 = set16.prefetch(buffer_size=AUTOTUNE)\n",
        "    set17 = set17.prefetch(buffer_size=AUTOTUNE)\n",
        "    set18 = set18.prefetch(buffer_size=AUTOTUNE)\n",
        "    return set1,set2,set3,set4,set5,set6,set7,set8,set9,set10,set11,set12,set13,set14,set15,set16,set17,set18\n",
        "\n",
        "set1,set2,set3,set4,set5,set6,set7,set8,set9,set10,set11,set12,set13,set14,set15,set16,set17,set18 = model2_init_sets(BATCH_SIZE, IMG_SIZE, AUTOTUNE)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ4SsxV0Q24Y"
      },
      "source": [
        "## **NEXT:** get code from vs code to run through all sets and merge with this goal: save all logits and actual labels for each dataset in a dataframe which will be 18*2=36 columns and however many samples for rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2FohnCiVibU"
      },
      "source": [
        "Define function for generating logits from dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1i50PBHVgzI"
      },
      "source": [
        "def get_logits(dataset, model):\n",
        "    all_pred=tf.zeros([], tf.float64) #initialize array to hold all prediction logits (single element)\n",
        "    all_labels=tf.zeros([], tf.float64) #initialize array to hold all actual labels (single element)\n",
        "    for image_batch, label_batch in dataset.as_numpy_iterator():\n",
        "        predictions = model.predict_on_batch(image_batch).flatten() #run batch through model and return logits\n",
        "        all_pred = tf.experimental.numpy.append(all_pred, predictions)\n",
        "        all_labels = tf.experimental.numpy.append(all_labels, label_batch)\n",
        "    #tf.size(all_pred) #1335 elements, 1334 images + 1 placeholder 0 at beginning\n",
        "    all_pred = all_pred[1:]\n",
        "    all_labels = all_labels[1:]\n",
        "    return all_pred,all_labels"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOAy8vYhWq3X"
      },
      "source": [
        "NEXT: Set random seed to see about reproducing the same logit predictions each time by starting with the same batch first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8asmKVbtWqYE"
      },
      "source": [
        "all_pred, all_labels = get_logits(set1,model)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8HwdbIbsYLo",
        "outputId": "619e1b15-a736-4cf5-85aa-09ff227c033d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_pred.numpy()[:5] #first five logits, of 1000"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.05096734,  2.50202107, -3.50221658, -1.16470253, -1.67171085])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33UGFwoutRHZ",
        "outputId": "cb096489-be2d-432c-a366-255271413c24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_pred.numpy().shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAIVq7utVQf2"
      },
      "source": [
        "all_sets = [set1,set2,set3,set4,set5,set6,set7,set8,set9,set10,set11,set12,set13,set14,set15,set16,set17,set18]\n",
        "for dataset in all_sets: #run for all sets:\n",
        "    df, df_results, all_avg_acc, avg_accuracy = get_conf_acc_dataset(dataset, model, logit_thres_min, logit_thres_max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E54Mm44j35qn"
      },
      "source": [
        "### 3: Test different datasets\n",
        "Setup and run the test to calculate both Tf's accuracy and my naive calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkD8QOuqB93g"
      },
      "source": [
        "def test_dataset(current_set):\n",
        "  all_acc=tf.zeros([], tf.float64) #initialize array to hold all accuracy indicators (single element)\n",
        "  loss, acc = model.evaluate(current_set) #now test the model's performance on the test set\n",
        "  for image_batch, label_batch in current_set.as_numpy_iterator():\n",
        "      predictions = model.predict_on_batch(image_batch).flatten() #run batch through model and return logits\n",
        "      predictions = tf.nn.sigmoid(predictions) #apply sigmoid activation function to transform logits to [0,1]\n",
        "      predictions = tf.where(predictions < 0.5, 0, 1) #round down or up accordingly since it's a binary classifier\n",
        "      accuracy = tf.where(tf.equal(predictions,label_batch),1,0) #correct is 1 and incorrect is 0\n",
        "      all_acc = tf.experimental.numpy.append(all_acc, accuracy)\n",
        "  all_acc = all_acc[1:]  #drop first placeholder element\n",
        "  avg_acc = tf.reduce_mean(all_acc)\n",
        "  print('My Accuracy:', avg_acc.numpy()) \n",
        "  print('Tf Accuracy:', acc) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAX3FAtK4UeJ",
        "outputId": "7b2c6e98-d46a-446c-c7ba-8e4e422a4045"
      },
      "source": [
        "test_dataset(set1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 16s 451ms/step - loss: 0.6922 - accuracy: 0.5030\n",
            "My Accuracy: 0.543\n",
            "Tf Accuracy: 0.503000020980835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAodEKlZ43Pj"
      },
      "source": [
        "My accuracy yielded a 54.3% accuracy while Tensorflow's yielded 50.30%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSODie-t4xsu",
        "outputId": "329a8276-3050-4697-b4f0-2b0d2f946916"
      },
      "source": [
        "test_dataset(set4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 15s 449ms/step - loss: 0.6854 - accuracy: 0.5040\n",
            "My Accuracy: 0.558\n",
            "Tf Accuracy: 0.5040000081062317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAezusdk5EbK"
      },
      "source": [
        "My accuracy yielded a 54.3% accuracy while Tensorflow's yielded 50.30%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_bgBCBz5F2P",
        "outputId": "94b66b0f-d232-4f0e-f75e-7b5b3ae0bf87"
      },
      "source": [
        "test_dataset(set10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 14s 439ms/step - loss: 0.6817 - accuracy: 0.5040\n",
            "My Accuracy: 0.552\n",
            "Tf Accuracy: 0.5040000081062317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOYvmJPi5Lgd"
      },
      "source": [
        "My accuracy yielded a 55.2% accuracy while Tensorflow's yielded 50.40%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVaDM4XL5MFL",
        "outputId": "4026427a-8596-4087-b6a0-db89c7c5a769"
      },
      "source": [
        "test_dataset(set15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 14s 439ms/step - loss: 0.1042 - accuracy: 0.9660\n",
            "My Accuracy: 0.987\n",
            "Tf Accuracy: 0.9660000205039978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0lAZ3K_5i8k"
      },
      "source": [
        "My accuracy yielded a 98.7% accuracy while Tensorflow's yielded 96.6%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiT1eVSh5lBQ",
        "outputId": "6e9c7c4f-5f8d-4534-b45d-27ceac15a306"
      },
      "source": [
        "test_dataset(set16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 14s 434ms/step - loss: 0.6551 - accuracy: 0.5200\n",
            "My Accuracy: 0.648\n",
            "Tf Accuracy: 0.5199999809265137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl3snT2d5s4i"
      },
      "source": [
        "My accuracy yielded a 64.8% accuracy while Tensorflow's yielded ~52%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOc2iZob5vlC",
        "outputId": "918fa484-d34a-4dcd-cc26-ef98e9ea81ee"
      },
      "source": [
        "test_dataset(set17)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 15s 451ms/step - loss: 0.5398 - accuracy: 0.6750\n",
            "My Accuracy: 0.832\n",
            "Tf Accuracy: 0.675000011920929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4qiHWRv57nC"
      },
      "source": [
        "My accuracy yielded a 83.2% accuracy while Tensorflow's yielded 67.5%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjKHej5f59Tl",
        "outputId": "d030c93c-15b0-4fb3-b719-0e139c990edc"
      },
      "source": [
        "test_dataset(set18)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 15s 457ms/step - loss: 0.0091 - accuracy: 1.0000\n",
            "My Accuracy: 1.0\n",
            "Tf Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7EYd0zK6JXd"
      },
      "source": [
        "My accuracy yielded 100% accuracy while Tensorflow's yielded 100%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sRRB5md6WYS"
      },
      "source": [
        "Dataset 17 showed the biggest disparity and dataset 18 was the only set that showed the same accuracy in both calculations"
      ]
    }
  ]
}