# Confidence Dissociation in Humans and Neural Networks
## Methods
- All code used in this project can be found on its Github page: https://github.com/thor4/neuralnets/tree/master/projects/1-CNN
- Datasets, models, and results were stored on its Open Science Foundation (OSF) project page located here: https://osf.io/mcwxb/
- A video explanation of this project was recorded via Zoom and is available on the OSF page here: https://osf.io/eux5k/
- The Tensorflow Python library was used extensively throughout this project to train and test the neural network models. The numpy, pandas, and matplotlib libraries were also used to process, analyze, and plot data. 
### Gabor generation
Gabor patches were generated using MATLAB scripts and Psychtoolbox. All Gabors were generated with a standard width of 169 pixels. They used a default of six standard deviations of the Gaussian curve defining the patch contained within the square. The contrast, which controlled the difference between darker and lighter parts of the patch, was either 0.3, 0.45, or 1 (maximum difference). The contrast noise was always set to 1, the maximum amount. The magnitude of the period of the Gabor patch’s grating was always set to 0.5 standard deviation units. The orientation of the Gabors were tilted either clockwise or counterclockwise from 45 degrees. The magnitude of the tilts ranged from [0.05,2] radians for the Vanilla Gabor model, while the range for the CIFAR10 Gabor model ranged from [0.5,4].
For the model training, Gabors were randomly chosen from the relevant model’s training range and paired with each of the three contrasts. 
For the tilt search, Gabors were created using statically defined tilts, evenly spaced within the relevant model’s training range, including the endpoints. They all used a contrast of 0.45.
### Model training
The Gabors used to train the models were split into two classes: tilts that were clockwise from 45 degrees and those that were counterclockwise from 45 degrees. For the Vanilla Gabor model, the tilts were randomly chosen from a range between [0.05,2], inclusive. The range was [0.5,4], inclusive, for the CIFAR10 Gabor model. 3,500 Gabors were generated for each of three contrasts (0.30, 0.45, and 1) and for each class. This yielded 21,000 total Gabors. They were divided into 13,998 training images, 5,598 validation images, and 1,404 testing images. The Vanilla Gabor training dataset was uploaded to the OSF project page under the datasets folder and named model_training-range_con_rand_tilt_0_05-2.zip. The CIFAR10 Gabor training dataset was also uploaded to OSF and named model_training-range_con_rand_tilt_0_5-4.zip.
Prior to fine-tuning on the Gabor images, the CIFAR10 Gabor model was trained using the CIFAR10 dataset. It is comprised of 60,000 images in 10 classes, with 6,000 images in each class. It is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive. A sparse categorical cross entropy loss function was used across 10 training epochs. After training, the last dense layer with 10 classes was replaced with a dense layer consisting of a single output for the Gabor classification task. At this point it was ready to be fine-tuned by training on the Gabors. This process was repeated 30 times to enhance statistical accuracy.
The architecture of each model was comprised of two convolutional layers and two dense layers. Images were first resized to 160x160 and rescaled to ensure all values were between 0 and 1. After these operations, they went through the first convolutional layer followed by two max pooling operations, a second convolutional layer, and another max pooling operation. Next, the network was flattened to a single dimension and the images proceeded through two final dense layers resulting in a single output. The outputs are called logits. Negative logits represented the counterclockwise class and positive logits represented the clockwise class.
Gabor images were grouped into batches of 32 for training with an image size of 160x160. A validation set was used during the training process to provide a model accuracy and loss after each epoch. It can be useful in gauging whether the model is overfitting. For instance, if the validation metrics stop improving while the training metrics continue to improve, that may signal overfitting has begun. 
The training, validation, and test sets were configured to optimize performance using buffered prefetching, which loads images in advance.
Each model was trained on the Gabors using a binary cross entropy loss function. The Vanilla Gabor model was trained for just two epochs, while the CIFAR10 Gabor model was fine-tuned using 10 epochs. This process was repeated 30 times so each model’s results could converge on an average value within 1 SEM. Each model was saved and uploaded to OSF. Models that did not achieve at least a 55% accuracy were replaced to ensure only models that performed convincingly above chance were used in the analysis. The CIFAR10 Gabor models were saved under the models folder in the OSF project and named cifar10_gabors-models.zip. The Vanilla Gabor models were named van_gabor-models.zip.
### Tilt search
It was important that the model’s performance tracked that of the human behavioral data. So, 25 evenly spaced tilts from each model’s training range were chosen to test which generated accuracies of around 60%, 70%, and 80%. These were most like human-level performance. 2,000 Gabors were generated for each of the 25 tilts and for each class with an 0.45 contrast. 
Each tilt’s Gabors were passed through the model to generate an accuracy. The tilts that generated accuracies most like human performance were 0.05 (57.15%), 0.1313 (67.75%), and 0.2125 (78.10%) degrees for the Vanilla Gabor model and 1.0833 (59.35%), 2.3958 (69.13%), and 4 (78.63%) degrees for the CIFAR10 Gabor model.
### Generate results
After identifying tilts that matched human performance, they were processed through the model in a more rigorous process to generate results. 2,000 new Gabors were generated for each of the three tilt and three contrast combinations for each class, yielding nine datasets. This resulted in 36,000 total images. Different Gabors were generated for each model.
The datasets were processed through all 30 networks previously generated for each model. Results listing each model’s tilt/contrast average accuracy and confidence were saved as separate excel files for each model. Each model’s logits, labels, predictions, accuracy and tilt/contrast combinations were also saved as separate excel files. The results spreadsheets were zipped and uploaded to OSF for subsequent use in plotting the figure. The Vanilla Gabor model’s file was named van_gabor-models-results.zip and the CIFAR10 Gabor model’s file was named cifar10_gabors-models-results.zip.
