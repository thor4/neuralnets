{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.6 64-bit ('base': conda)"
    },
    "interpreter": {
      "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
    },
    "colab": {
      "name": "step5-test_iterations-van_gabor.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thor4/neuralnets/blob/master/projects/1-CNN/step5-test_iterations-van_gabor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nFaKIvYizD9"
      },
      "source": [
        "# Test Iterations\n",
        "--- \n",
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apuqn65zdSGP",
        "outputId": "c58d5f61-695c-41e2-82ef-988ca7abb0e5"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar  2 18:04:59 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSiAH1midWml",
        "outputId": "cec777b2-88a1-486c-f9a3-fb31f4fcc45b"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icB2TxuLizD_"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy.special import expit #import sigmoid func\n",
        "tf.random.set_seed(42) #set random seed for reproducibility"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load and prepare the data\n",
        "---"
      ],
      "metadata": {
        "id": "0gjzdi2RN9G9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the models\n",
        "\n",
        "These models were created using the `step4_train_iterations` Jupyter notebook. Run the cell to download a zip file from OSF then extract its contents into the newly created directory.\n",
        "\n",
        "vanilla gabor models: `content/van_gabor-model_#/`\n",
        "\n",
        "where # = [0,29]"
      ],
      "metadata": {
        "id": "0eOvkcJgO8Sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download + unzip models\n",
        "\n",
        "import requests, os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "print(\"Start downloading and unzipping vanilla models trained on Gabors...\")\n",
        "name = 'van_gabor_models'\n",
        "fname = f\"{name}.zip\"\n",
        "url = f\"https://osf.io/wzy76/download\" #osf share link\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "with open(fname, 'wb') as fh:\n",
        "  fh.write(r.content) #download file\n",
        "\n",
        "with ZipFile(fname, 'r') as zfile:\n",
        "  zfile.extractall() #extract contents\n",
        "\n",
        "if os.path.exists(fname):\n",
        "  os.remove(fname) #delete zip file\n",
        "else:\n",
        "  print(f\"The file {fname} does not exist\")\n",
        "\n",
        "print(\"Download completed.\")\n",
        "!unzip 'van_gabor-model_*.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FL-d1C2O73N",
        "outputId": "43b1fe35-fbc9-4b62-9b1f-42d4593dfef0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading and unzipping vanilla models trained on Gabors...\n",
            "Download completed.\n",
            "Archive:  van_gabor-model_0.zip\n",
            "caution: filename not matched:  van_gabor-model_10.zip\n",
            "caution: filename not matched:  van_gabor-model_11.zip\n",
            "caution: filename not matched:  van_gabor-model_12.zip\n",
            "caution: filename not matched:  van_gabor-model_13.zip\n",
            "caution: filename not matched:  van_gabor-model_14.zip\n",
            "caution: filename not matched:  van_gabor-model_15.zip\n",
            "caution: filename not matched:  van_gabor-model_16.zip\n",
            "caution: filename not matched:  van_gabor-model_17.zip\n",
            "caution: filename not matched:  van_gabor-model_18.zip\n",
            "caution: filename not matched:  van_gabor-model_19.zip\n",
            "caution: filename not matched:  van_gabor-model_1.zip\n",
            "caution: filename not matched:  van_gabor-model_20.zip\n",
            "caution: filename not matched:  van_gabor-model_21.zip\n",
            "caution: filename not matched:  van_gabor-model_22.zip\n",
            "caution: filename not matched:  van_gabor-model_23.zip\n",
            "caution: filename not matched:  van_gabor-model_24.zip\n",
            "caution: filename not matched:  van_gabor-model_25.zip\n",
            "caution: filename not matched:  van_gabor-model_26.zip\n",
            "caution: filename not matched:  van_gabor-model_27.zip\n",
            "caution: filename not matched:  van_gabor-model_28.zip\n",
            "caution: filename not matched:  van_gabor-model_29.zip\n",
            "caution: filename not matched:  van_gabor-model_2.zip\n",
            "caution: filename not matched:  van_gabor-model_3.zip\n",
            "caution: filename not matched:  van_gabor-model_4.zip\n",
            "caution: filename not matched:  van_gabor-model_5.zip\n",
            "caution: filename not matched:  van_gabor-model_6.zip\n",
            "caution: filename not matched:  van_gabor-model_7.zip\n",
            "caution: filename not matched:  van_gabor-model_8.zip\n",
            "caution: filename not matched:  van_gabor-model_9.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Delete all zip files to save space.\n",
        "dir_name = os.getcwd()\n",
        "test = os.listdir(dir_name)\n",
        "for item in test:\n",
        "    if item.endswith(\".zip\"):\n",
        "        os.remove(os.path.join(dir_name, item))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TzKpNreO0WZh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model\n",
        "Next, we load the models using Tensorflow"
      ],
      "metadata": {
        "id": "daTO60vGQzo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load models\n",
        "model0 = tf.keras.models.load_model('van_gabor-model_0')\n",
        "model1 = tf.keras.models.load_model('van_gabor-model_1')\n",
        "model2 = tf.keras.models.load_model('van_gabor-model_2')\n",
        "model3 = tf.keras.models.load_model('van_gabor-model_3')\n",
        "model4 = tf.keras.models.load_model('van_gabor-model_4')\n",
        "model5 = tf.keras.models.load_model('van_gabor-model_5')\n",
        "model6 = tf.keras.models.load_model('van_gabor-model_6')\n",
        "model7 = tf.keras.models.load_model('van_gabor-model_7')\n",
        "model8 = tf.keras.models.load_model('van_gabor-model_8')\n",
        "model9 = tf.keras.models.load_model('van_gabor-model_9')\n",
        "model10 = tf.keras.models.load_model('van_gabor-model_10')\n",
        "model11 = tf.keras.models.load_model('van_gabor-model_11')\n",
        "model12 = tf.keras.models.load_model('van_gabor-model_12')\n",
        "model13 = tf.keras.models.load_model('van_gabor-model_13')\n",
        "model14 = tf.keras.models.load_model('van_gabor-model_14')\n",
        "model15 = tf.keras.models.load_model('van_gabor-model_15')\n",
        "model16 = tf.keras.models.load_model('van_gabor-model_16')\n",
        "model17 = tf.keras.models.load_model('van_gabor-model_17')\n",
        "model18 = tf.keras.models.load_model('van_gabor-model_18')\n",
        "model19 = tf.keras.models.load_model('van_gabor-model_19')\n",
        "model20 = tf.keras.models.load_model('van_gabor-model_20')\n",
        "model21 = tf.keras.models.load_model('van_gabor-model_21')\n",
        "model22 = tf.keras.models.load_model('van_gabor-model_22')\n",
        "model23 = tf.keras.models.load_model('van_gabor-model_23')\n",
        "model24 = tf.keras.models.load_model('van_gabor-model_24')\n",
        "model25 = tf.keras.models.load_model('van_gabor-model_25')\n",
        "model26 = tf.keras.models.load_model('van_gabor-model_26')\n",
        "model27 = tf.keras.models.load_model('van_gabor-model_27')\n",
        "model28 = tf.keras.models.load_model('van_gabor-model_28')\n",
        "model29 = tf.keras.models.load_model('van_gabor-model_29')\n",
        "model0.summary() #verify architecture"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKPIXG6dQ1yJ",
        "outputId": "3f50c1f0-1769-4288-ecb6-6ccf7c6a7453"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rescaling_2 (Rescaling)     (None, 160, 160, 1)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 158, 158, 160)     1600      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 79, 79, 160)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 39, 39, 160)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 37, 37, 80)        115280    \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 18, 18, 80)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 25920)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 80)                2073680   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 81        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,190,641\n",
            "Trainable params: 2,190,641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwDFLrffizEC"
      },
      "source": [
        "Download the 9 test datasets from OSF and extract the contents into the newly created directory: `content/datasets/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ss70gaYoa7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f8b40a-c6ee-4b02-b13d-593839e79223",
        "cellView": "form"
      },
      "source": [
        "# @title Download datasets to test the model\n",
        "\n",
        "print(\"Start downloading and unzipping `9 datasets`...\")\n",
        "name = 'tilt_contrast-van_gabor'\n",
        "fname = f\"{name}.zip\"\n",
        "url = f\"https://osf.io/wc3nr/download\" #osf share link\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "with open(fname, 'wb') as fh:\n",
        "  fh.write(r.content) #download file\n",
        "\n",
        "with ZipFile(fname, 'r') as zfile:\n",
        "  zfile.extractall(\"datasets\") #extract contents\n",
        "\n",
        "if os.path.exists(fname):\n",
        "  os.remove(fname) #delete zip file\n",
        "else:\n",
        "  print(f\"The file {fname} does not exist\")\n",
        "\n",
        "print(\"Download completed.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading and unzipping `9 datasets`...\n",
            "Download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load all 9 training sets and use prefetch to streamline image loading."
      ],
      "metadata": {
        "id": "n_ege-imBdpK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXaUncrIizED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70e8b45-1941-4e5d-cb42-b26245e83556",
        "cellView": "form"
      },
      "source": [
        "# @title Load datasets into tensorflow\n",
        "\n",
        "BATCH_SIZE = 32 \n",
        "IMG_SIZE = (160, 160) #forces a resize from 170x170 since MobileNetV2 has weights only for certain sizes\n",
        "AUTOTUNE = tf.data.AUTOTUNE #prompts the tf.data runtime to tune the value dynamically at runtime\n",
        "def model2_init_sets(BATCH_SIZE, IMG_SIZE, AUTOTUNE):\n",
        "    curr_dir = os.getcwd() \n",
        "    set1_dir = os.path.join(curr_dir, 'datasets/t_0_05-c_0_3')\n",
        "    set2_dir = os.path.join(curr_dir, 'datasets/t_0_05-c_0_45')\n",
        "    set3_dir = os.path.join(curr_dir, 'datasets/t_0_05-c_1')\n",
        "    set4_dir = os.path.join(curr_dir, 'datasets/t_0_1313-c_0_3')\n",
        "    set5_dir = os.path.join(curr_dir, 'datasets/t_0_1313-c_0_45')\n",
        "    set6_dir = os.path.join(curr_dir, 'datasets/t_0_1313-c_1')\n",
        "    set7_dir = os.path.join(curr_dir, 'datasets/t_0_2125-c_0_3')\n",
        "    set8_dir = os.path.join(curr_dir, 'datasets/t_0_2125-c_0_45')\n",
        "    set9_dir = os.path.join(curr_dir, 'datasets/t_0_2125-c_1')\n",
        "    set1 = image_dataset_from_directory(set1_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale') #2000 images 2 classes\n",
        "    set2 = image_dataset_from_directory(set2_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')    \n",
        "    set3 = image_dataset_from_directory(set3_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set4 = image_dataset_from_directory(set4_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set5 = image_dataset_from_directory(set5_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set6 = image_dataset_from_directory(set6_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set7 = image_dataset_from_directory(set7_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale') \n",
        "    set8 = image_dataset_from_directory(set8_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    set9 = image_dataset_from_directory(set9_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE, color_mode='grayscale')\n",
        "    class_names = set1.class_names #extract class names loading function inferred from subdir's\n",
        "    set1 = set1.prefetch(buffer_size=AUTOTUNE) \n",
        "    set2 = set2.prefetch(buffer_size=AUTOTUNE) \n",
        "    set3 = set3.prefetch(buffer_size=AUTOTUNE) \n",
        "    set4 = set4.prefetch(buffer_size=AUTOTUNE) \n",
        "    set5 = set5.prefetch(buffer_size=AUTOTUNE) \n",
        "    set6 = set6.prefetch(buffer_size=AUTOTUNE) \n",
        "    set7 = set7.prefetch(buffer_size=AUTOTUNE) \n",
        "    set8 = set8.prefetch(buffer_size=AUTOTUNE) \n",
        "    set9 = set9.prefetch(buffer_size=AUTOTUNE) \n",
        "    return set1,set2,set3,set4,set5,set6,set7,set8,set9,class_names\n",
        "\n",
        "set1,set2,set3,set4,set5,set6,set7,set8,set9,class_names = model2_init_sets(BATCH_SIZE, IMG_SIZE, AUTOTUNE)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuZc8G7k-Adt"
      },
      "source": [
        "There are 4,000 images per set, 2,000 per class, 36,000 total across sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9mKkQwPizEF"
      },
      "source": [
        "### 2. Generate logits\n",
        "Next, we can define a function for processing a dataset through a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66NBuNRpizEG"
      },
      "source": [
        "def process_dataset(dataset, model):\n",
        "    all_logits=tf.zeros([], tf.float64) #initialize array to hold all prediction logits (single element)\n",
        "    all_labels=tf.zeros([], tf.float64) #initialize array to hold all actual labels (single element)\n",
        "    for image_batch, label_batch in dataset.as_numpy_iterator():\n",
        "        predictions = model.predict_on_batch(image_batch).flatten() #run batch through model and return logits\n",
        "        all_logits = tf.experimental.numpy.append(all_logits, predictions)\n",
        "        all_labels = tf.experimental.numpy.append(all_labels, label_batch)\n",
        "    #tf.size(all_pred) #1335 elements, 1334 images + 1 placeholder 0 at beginning\n",
        "    all_logits = all_logits[1:] #remove placeholder at beginning\n",
        "    all_labels = all_labels[1:]\n",
        "    all_logits_sig = expit(all_logits) #sigmoid-transform the logits\n",
        "    all_pred = np.where((all_logits_sig < 0.5), 0, 1) #replace predictions with 0 or 1\n",
        "    all_acc = np.where((all_pred == all_labels), 1, 0) #decide whether pred = label\n",
        "    return all_logits,all_labels,all_pred,all_acc"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's define a function for processing all 9 datasets through a model. In it, we will stack each sets' 4,000 logits in a single dataframe, `df_set`. The resulting dataframe will have 36,000 logits resulting from vertically stacking the logits for set1, set 2, .., set9. Additionally, we will calculate the average raw confidence and accuracy scores for each tilt/contrast combination and save this in a separate dataframe called `df_model_results`."
      ],
      "metadata": {
        "id": "YQnXLPYEUkBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_model(model,set1,set2,set3,set4,set5,set6,set7,set8,set9):\n",
        "    df_set = pd.DataFrame(columns=['Logits','Labels','Predictions','Accuracy','Tilt',\n",
        "                              'Contrast']) #init dataframe\n",
        "    df_model_results = pd.DataFrame(columns=['Accuracy','Confidence']) #init dataframe\n",
        "    all_sets = [set1,set2,set3,set4,set5,set6,set7,set8,set9]\n",
        "    all_tilts = [0.05, 0.05, 0.05, 0.1313, 0.1313, 0.1313, 0.2125, 0.2125, 0.2125]\n",
        "    all_contrasts = [0.3, 0.45, 1, 0.3, 0.45, 1, 0.3, 0.45, 1]\n",
        "    for idx, dataset in enumerate(all_sets): #run for all sets:\n",
        "        tilt = all_tilts[idx]\n",
        "        contrast = all_contrasts[idx]\n",
        "        all_logits,all_labels,all_pred,all_acc = process_dataset(dataset, model)\n",
        "        tilts = np.repeat(tilt, all_pred.size)\n",
        "        contrasts = np.repeat(contrast, all_pred.size)\n",
        "        df_set = pd.concat([df_set, pd.DataFrame({'Logits':all_logits.numpy(),\n",
        "                                          'Labels':all_labels.numpy(),\n",
        "                                          'Predictions':all_pred,'Accuracy':all_acc,\n",
        "                                          'Tilt':tilts,'Contrast':contrasts})], \n",
        "                      axis=0, ignore_index=True) #append logits, labels,etc to dataframe\n",
        "        acc = all_acc.mean() #calculate avg accuracy\n",
        "        conf = np.absolute(all_logits.numpy()).mean()\n",
        "        df_model_results = pd.concat([df_results, pd.DataFrame({'Accuracy':[acc],\n",
        "                                                    'Confidence':[conf],\n",
        "                                                    'Tilt':tilt,'Contrast':contrast})], \n",
        "                          axis=0, ignore_index=True) #append acc & conf to dataframe\n",
        "    return df_set,df_model_results"
      ],
      "metadata": {
        "id": "qET2WIvbU2Qg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STOPPED HERE, NEED TO PROCESS A MODEL AND EXPORT DF'S AS EXCEL FILES AND SAVE THEM TO DISK EACH TIME WITH DYNAMIC NAMING CONVENTION"
      ],
      "metadata": {
        "id": "g2ElAdiG2gY-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dLeIlRVizEI"
      },
      "source": [
        "\n",
        "### Standardize the data\n",
        "The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network; in general we should seek to make our input values small. We will standardize values to be in the `[0, 1]` range by using the `tf.keras.layers.experimental.preprocessing.Rescaling` layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6glOdQuwizEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d5901c-9949-49c8-d264-6064c99a99eb"
      },
      "source": [
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "normalized_ds = train_dataset.map(lambda x, y: (normalization_layer(x), y)) #apply normalization to dataset\n",
        "image_batch, labels_batch = next(iter(normalized_ds)) #extract one batch from the norm'd dataset\n",
        "first_image = image_batch[0] #view first image from the batch\n",
        "image_batch2, labels_batch2 = next(iter(train_dataset)) #extract one batch from the original dataset\n",
        "first_image2 = image_batch2[0] #view first image from the batch\n",
        "print(f\"before normalization:\",np.min(first_image2),np.max(first_image2),\"\\n\",\n",
        "      f\"after normalization:\",np.min(first_image),np.max(first_image)) #notice the pixels values are now in `[0,1]`."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before normalization: 0.7294922 254.38867 \n",
            " after normalization: 0.007877605 0.9995214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKjtYqv2izEL"
      },
      "source": [
        "Now that we are convinced the normalization works, we will include it as the first layer inside our model definition below to simplify deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpDT6HNwizEM"
      },
      "source": [
        "### Setup buffered prefetching\n",
        "We can configure the dataset for performance using buffered prefetching to load images from disk without having I/O become blocking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntH5gPNaizEN"
      },
      "source": [
        "def prefetch(train_dataset, validation_dataset, test_dataset):\n",
        "    AUTOTUNE = tf.data.AUTOTUNE #prompts the tf.data runtime to tune the value dynamically at runtime\n",
        "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE) #will prefetch an optimal number of batches\n",
        "    validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE) #will prefetch an optimal number of batches\n",
        "    test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE) #will prefetch an optimal number of batches\n",
        "    return train_dataset,validation_dataset,test_dataset,AUTOTUNE\n",
        "\n",
        "train_dataset, validation_dataset, test_dataset, AUTOTUNE = prefetch(train_dataset, validation_dataset, test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwq6kTQ_izEO"
      },
      "source": [
        "### Create the model\n",
        "\n",
        "The first 3 lines of code below instantiate the Sequential model definition, declare the input shape of the images then apply the normalization layer.\n",
        "\n",
        "As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. color_channels refers to (R,G,B). We will configure the CNN to process inputs of shape (160, 160, 3), which is the format of our Gabors. We can do this by passing the argument `input_shape` to the first layer titled `InputLayer`.\n",
        "\n",
        "The next 5 lines of code below define the convolutional base using a common pattern: a stack of [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [MaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers.\n",
        "\n",
        "To complete the model, we will feed the last output tensor from the convolutional base (of shape (36, 36, 320)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, we will flatten (or unroll) the 3D output to 1D, then add one or more Dense layers on top. Our dataset has 2 output classes, so we use a final Dense layer with a single logit output prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__vLsH7VizEO"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.InputLayer(input_shape=(160,160,1)))\n",
        "model.add(layers.experimental.preprocessing.Rescaling(1./255)) #normalization layer\n",
        "model.add(layers.Conv2D(160, (3, 3), activation='relu', input_shape=(160, 160, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(80, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten()) #flatten from 3d output to 1d\n",
        "model.add(layers.Dense(80, activation='relu'))\n",
        "model.add(layers.Dense(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOpf3jmLizEO"
      },
      "source": [
        "Let's display the architecture of the model so far:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOrtgAyEizEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c87aaece-1b99-4c24-c34c-2f0554140c53"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rescaling_1 (Rescaling)     (None, 160, 160, 1)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 158, 158, 160)     1600      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 79, 79, 160)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 39, 39, 160)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 37, 37, 80)        115280    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 18, 18, 80)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25920)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 80)                2073680   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 81        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,190,641\n",
            "Trainable params: 2,190,641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkoPZTuFizEP"
      },
      "source": [
        "Above, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as we go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 160 or 80). \n",
        "\n",
        "Finally, we see that (18, 18, 80) outputs were flattened into vectors of shape (25920) before going through two Dense layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPjHrtWmizEQ"
      },
      "source": [
        "### Compile and train the model\n",
        "\n",
        "We will define the learning rate and use the BinaryCrossEntropy loss since the model provides a single linear output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC2an-yuizEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b332ba47-ce3a-473d-da96-6b592336ec32"
      },
      "source": [
        "base_learning_rate = 0.0001 #define the learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate), \n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_dataset, \n",
        "                    epochs=2, \n",
        "                    validation_data=validation_dataset) #tests against validation dataset after each iteration"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "438/438 [==============================] - 90s 178ms/step - loss: 0.6829 - accuracy: 0.5011 - val_loss: 0.6202 - val_accuracy: 0.5972\n",
            "Epoch 2/2\n",
            "438/438 [==============================] - 74s 168ms/step - loss: 0.2872 - accuracy: 0.8616 - val_loss: 0.1374 - val_accuracy: 0.9405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7mHvdZqizER"
      },
      "source": [
        "### Evaluate the model\n",
        "Let's make a plot to visualize how the training and validation accuracy improves with each epoch. We will also evaluate the model against the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qedbSxLEizER",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "077572e0-eed9-4f3d-9c54-c2d5c43d1585"
      },
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_dataset, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44/44 - 4s - loss: 0.1436 - accuracy: 0.9323 - 4s/epoch - 82ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV7RxRg4oEJH"
      },
      "source": [
        "Nicely done! The model achieved a 94.05% accuracy on the validation set and a 93.23% accuracy on the test set after 2 epochs. Now let's save our vanilla CNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2bR7_R-oX7j",
        "outputId": "2ffd28bf-de7e-4e3b-cb12-b49c3c568118"
      },
      "source": [
        "model.save('van_gabor')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: van_gabor/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJOIkslgz_s8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ed73f4a-9bd9-40f9-8777-7344649643a8"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/model_training-range_con_rand_tilt_0_05-2'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY_BYpG1pJe7",
        "outputId": "bac5f1a8-25de-4766-8a28-f4329c3e88ef"
      },
      "source": [
        "! zip -r van_gabor_model.zip van_gabor/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: van_gabor/ (stored 0%)\n",
            "  adding: van_gabor/assets/ (stored 0%)\n",
            "  adding: van_gabor/keras_metadata.pb (deflated 90%)\n",
            "  adding: van_gabor/saved_model.pb (deflated 88%)\n",
            "  adding: van_gabor/variables/ (stored 0%)\n",
            "  adding: van_gabor/variables/variables.index (deflated 64%)\n",
            "  adding: van_gabor/variables/variables.data-00000-of-00001 (deflated 46%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRC-BHczz7SA"
      },
      "source": [
        "/content/model2_training/18kim_range_vanilla_v2/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}